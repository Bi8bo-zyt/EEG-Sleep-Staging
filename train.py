import os
import torch
import numpy as np
import time
import copy
from itertools import islice
from datetime import datetime
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from data_loaders import NestedCVSplitter, create_loaders
from models.model import AttnSleep, FocalLoss

# é…ç½®å‚æ•°
config = {
    'data_dir': r'/home/Wsh/ZYT/EEG-Sleep-Staging/data',
    'checkpoint_dir': r'/home/Wsh/ZYT/EEG-Sleep-Staging/checkpoints',  # æ–°å¢æ£€æŸ¥ç‚¹ç›®å½•
    'n_outer_folds': 5,
    'epochs': 50,
    'batch_size': 256,
    'patience': 10,
    'lr': 3e-4,
    'weight_decay': 1e-3,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'class_weights': [1.0, 5.0, 1.0, 2.0, 3.0],  # W,N1,N2,N3,REM
    'focal_loss': True,
    'gamma': 2,
    'seed': 42,
    'resume': True  # æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­è®­
}


def save_checkpoint(state, filename='checkpoint.pth'):
    """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
    torch.save(state, os.path.join(config['checkpoint_dir'], filename))
    print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")


def load_checkpoint():
    """åŠ è½½æœ€è¿‘çš„æ£€æŸ¥ç‚¹"""
    try:
        # è·å–æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoints = [f for f in os.listdir(config['checkpoint_dir']) if f.endswith('.pth')]
        if not checkpoints:
            return None
        latest = max(checkpoints, key=lambda x: os.path.getmtime(os.path.join(config['checkpoint_dir'], x)))

        checkpoint = torch.load(os.path.join(config['checkpoint_dir'], latest))
        print(f"ğŸ” ä»æ£€æŸ¥ç‚¹æ¢å¤: {latest}")
        return checkpoint
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
        return None


def main():
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs(config['checkpoint_dir'], exist_ok=True)

    # åˆå§‹åŒ–äº¤å‰éªŒè¯
    splitter = NestedCVSplitter(config['data_dir'],
                                n_splits=config['n_outer_folds'],
                                seed=config['seed'])

    # å°è¯•åŠ è½½å…¨å±€æ£€æŸ¥ç‚¹
    global_state = None
    if config['resume']:
        global_state = load_checkpoint()

    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    all_results = []
    start_outer_fold = 0
    start_inner_fold = 0

    # æ¢å¤å…¨å±€çŠ¶æ€
    if global_state:
        all_results = global_state['results']
        start_outer_fold = global_state['outer_fold']
        start_inner_fold = global_state['inner_fold']
        print(f"â†©ï¸ ä»ç¬¬ {start_outer_fold + 1} æŠ˜å¤–å±‚, ç¬¬ {start_inner_fold + 1} æŠ˜å†…å±‚æ¢å¤")

    # å¤–å±‚äº¤å‰éªŒè¯å¾ªç¯
    for outer_fold in range(start_outer_fold, config['n_outer_folds']):
        print(f"\n=== Processing Outer Fold {outer_fold + 1}/{config['n_outer_folds']} ===")

        fold_data = splitter.get_fold(outer_fold)
        test_files = fold_data['test_files']

        best_inner_models = []

        # å†…å±‚äº¤å‰éªŒè¯
        for inner_idx, inner_split in enumerate(fold_data['train_val_splits']):
            if outer_fold == start_outer_fold and inner_idx < start_inner_fold:
                continue  # è·³è¿‡å·²å®Œæˆçš„inner fold

            start_inner_fold = 0  # é‡ç½®å†…å±‚ç´¢å¼•
            print(f"\n--- Inner Fold {inner_idx + 1}/{len(fold_data['train_val_splits'])} ---")

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader, val_loader, _ = create_loaders(
                inner_split['train_files'],
                inner_split['val_files'],
                test_files,
                config['batch_size']
            )

            # åˆå§‹åŒ–æ¨¡å‹
            model = AttnSleep().to(config['device'])
            optimizer = torch.optim.AdamW(model.parameters(),
                                          lr=config['lr'],
                                          weight_decay=config['weight_decay'])
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])

            # æ”¹è¿›çš„æŸå¤±å‡½æ•°
            if config['focal_loss']:
                # è½¬æ¢ç±»åˆ«æƒé‡ä¸ºTensor
                class_weights = torch.tensor(config['class_weights'],
                                             device=config['device'])
                criterion = FocalLoss(alpha=class_weights,
                                      gamma=config['gamma'])
            else:
                class_weights = torch.tensor(config['class_weights'],
                                             device=config['device'])
                criterion = nn.CrossEntropyLoss(weight=class_weights)

            # å°è¯•åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
            start_epoch = 0
            best_val_loss = float('inf')
            if config['resume']:
                checkpoint = load_checkpoint()
                if checkpoint and checkpoint['outer_fold'] == outer_fold and checkpoint['inner_fold'] == inner_idx:
                    model.load_state_dict(checkpoint['model'])
                    optimizer.load_state_dict(checkpoint['optimizer'])
                    scheduler.load_state_dict(checkpoint['scheduler'])
                    start_epoch = checkpoint['epoch'] + 1
                    best_val_loss = checkpoint['best_val_loss']
                    print(f"â†©ï¸ ä»ç¬¬ {start_epoch} è½®æ¢å¤è®­ç»ƒ")

            # è®­ç»ƒå¾ªç¯
            best_model = None
            patience_counter = 0
            for epoch in range(start_epoch, config['epochs']):
                start_time = time.time()

                # è®­ç»ƒæ­¥éª¤
                model.train()
                train_loss = 0
                for x, y in train_loader:
                    x = x.to(config['device'])
                    y = y.to(config['device'])

                    optimizer.zero_grad()
                    outputs = model(x)
                    loss = criterion(outputs, y)
                    loss.backward()
                    optimizer.step()

                    train_loss += loss.item() * x.size(0)

                # éªŒè¯æ­¥éª¤
                model.eval()
                val_loss = 0
                preds, truths = [], []
                with torch.no_grad():
                    for x, y in val_loader:
                        x = x.to(config['device'])
                        y = y.to(config['device'])

                        outputs = model(x)
                        loss = criterion(outputs, y)

                        val_loss += loss.item() * x.size(0)
                        preds.append(torch.argmax(outputs, 1).cpu())
                        truths.append(y.cpu())

                # è®¡ç®—æŒ‡æ ‡
                train_loss = train_loss / len(train_loader.dataset)
                val_loss = val_loss / len(val_loader.dataset)
                preds = torch.cat(preds).numpy()
                truths = torch.cat(truths).numpy()

                val_acc = accuracy_score(truths, preds)
                val_f1 = f1_score(truths, preds, average='macro')

                # å­¦ä¹ ç‡è°ƒæ•´
                scheduler.step()

                # ä¿å­˜æ£€æŸ¥ç‚¹
                checkpoint = {
                    'outer_fold': outer_fold,
                    'inner_fold': inner_idx,
                    'epoch': epoch,
                    'model': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'scheduler': scheduler.state_dict(),
                    'best_val_loss': best_val_loss,
                    'results': all_results
                }
                save_checkpoint(checkpoint, f'fold_{outer_fold}_inner_{inner_idx}_epoch_{epoch}.pth')

                # æ—©åœæœºåˆ¶
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model = copy.deepcopy(model.state_dict())
                    patience_counter = 0
                else:
                    patience_counter += 1

                # æ‰“å°è¿›åº¦
                print(f"Epoch {epoch + 1}/{config['epochs']} | "
                      f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
                      f"Acc: {val_acc:.4f} | F1: {val_f1:.4f} | Time: {time.time() - start_time:.2f}s")

                if patience_counter >= config['patience']:
                    print(f"â¹ï¸ ç¬¬ {epoch + 1} è½®è§¦å‘æ—©åœ")
                    break

            # ä¿å­˜æœ€ä½³å†…å±‚æ¨¡å‹
            best_inner_models.append(best_model)

            # ä¿å­˜å…¨å±€çŠ¶æ€
            global_state = {
                'outer_fold': outer_fold,
                'inner_fold': inner_idx,
                'results': all_results
            }
            save_checkpoint(global_state, 'global_state.pth')

        # å¤–å±‚æµ‹è¯•é›†è¯„ä¼°
        print("\n--- Testing on Outer Fold ---")
        _, _, test_loader = create_loaders(
            train_files=None,
            val_files=None,
            test_files=test_files,
            batch_size=config['batch_size']
        )

        # æ¨¡å‹é›†æˆ
        final_preds = []
        truths = None  # åˆå§‹åŒ–truthså˜é‡

        for model_state in best_inner_models:
            model.load_state_dict(model_state)
            model.eval()

            fold_preds = []
            fold_truths = []

            with torch.no_grad():
                for x, y in test_loader:
                    x = x.to(config['device'])
                    outputs = model(x)
                    fold_preds.append(torch.argmax(outputs, 1).cpu().numpy())
                    fold_truths.append(y.numpy())

            # ç¡®ä¿æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœä¸€è‡´
            fold_preds = np.concatenate(fold_preds)
            fold_truths = np.concatenate(fold_truths)

            if truths is None:
                truths = fold_truths
            else:
                # éªŒè¯ä¸åŒæ¨¡å‹çš„çœŸå®æ ‡ç­¾æ˜¯å¦ä¸€è‡´
                assert np.array_equal(truths, fold_truths), "ä¸åŒæ¨¡å‹çš„çœŸå®æ ‡ç­¾ä¸ä¸€è‡´"

            final_preds.append(fold_preds)

        # æŠ•ç¥¨é›†æˆ
        final_preds = np.stack(final_preds)
        ensemble_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), 0, final_preds)

        # æœ€ç»ˆéªŒè¯
        assert len(truths) == len(ensemble_preds), f"æ ‡ç­¾æ•°é‡ä¸åŒ¹é…: {len(truths)} vs {len(ensemble_preds)}"

        # è®¡ç®—æŒ‡æ ‡
        test_acc = accuracy_score(truths, ensemble_preds)
        test_f1 = f1_score(truths, ensemble_preds, average='macro')
        cm = confusion_matrix(truths, ensemble_preds)

        print(f"Outer Fold {outer_fold + 1} Results:")
        print(f"Accuracy: {test_acc:.4f}")
        print(f"Macro F1: {test_f1:.4f}")
        print("Confusion Matrix:")
        print(cm)

        all_results.append({
            'fold': outer_fold,
            'accuracy': test_acc,
            'f1': test_f1,
            'cm': cm
        })

        save_checkpoint({'results': all_results}, 'final_results.pth')

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n=== Final Results ===")
    accuracies = [res['accuracy'] for res in all_results]
    f1_scores = [res['f1'] for res in all_results]

    print(f"Average Accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}")
    print(f"Average Macro F1: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}")


if __name__ == '__main__':
    main()