import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold
import random

class SleepDataset(Dataset):
    def __init__(self, file_list, augment=True):
        self.augment = augment
        self.x_data = []
        self.y_data = []

        print(f"\nðŸ”§ æ­£åœ¨åŠ è½½ {len(file_list)} ä¸ªæ•°æ®æ–‡ä»¶...")
        valid_count = 0

        for idx, file in enumerate(file_list, 1):
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(file):
                    print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file}")
                    continue

                # åŠ è½½æ•°æ®
                with np.load(file) as data:
                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    if 'x' not in data or 'y' not in data:
                        print(f"  âš ï¸ æ–‡ä»¶æ ¼å¼é”™è¯¯: {file} ç¼ºå°‘x/yå­—æ®µ")
                        continue

                    x = data['x'].astype(np.float32)
                    y = data['y'].astype(np.int64)

                    # ç»Ÿä¸€ç»´åº¦å¤„ç†
                    if x.ndim == 3:
                        # æƒ…å†µ1: (n, 1, 3000) -> ç›´æŽ¥ä½¿ç”¨
                        if x.shape[1] == 1 and x.shape[2] == 3000:
                            pass
                        # æƒ…å†µ2: (n, 3000, 1) -> è½¬ç½®ä¸º(n, 1, 3000)
                        elif x.shape[1] == 3000 and x.shape[2] == 1:
                            x = x.transpose(0, 2, 1)
                        else:
                            raise ValueError(f"éžå¸¸è§„ä¸‰ç»´æ•°æ®: {x.shape}")
                    # å¤„ç†äºŒç»´æ•°æ® (n, 3000) -> (n, 1, 3000)
                    elif x.ndim == 2:
                        x = x[:, np.newaxis, :]
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç»´åº¦: {x.ndim}")

                    # æœ€ç»ˆå½¢çŠ¶éªŒè¯
                    if x.shape[1:] != (1, 3000):
                        print(f"  âš ï¸ ç»´åº¦æ ¡éªŒå¤±è´¥: {file} å½¢çŠ¶={x.shape}")
                        continue

                    # è½¬æ¢ä¸ºTensor
                    self.x_data.append(torch.from_numpy(x))
                    self.y_data.append(torch.from_numpy(y))
                    valid_count += 1

                    # è¿›åº¦æ˜¾ç¤º
                    if idx % 5 == 0:
                        print(f"  å·²åŠ è½½ {idx}/{len(file_list)} ä¸ªæ–‡ä»¶...")

            except Exception as e:
                print(f"  âŒ åŠ è½½é”™è¯¯ {file}: {str(e)}")
                continue

        # æ£€æŸ¥æœ‰æ•ˆæ•°æ®
        if valid_count == 0:
            raise RuntimeError(f"âŒ æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        self.x_data = torch.cat(self.x_data, dim=0)
        self.y_data = torch.cat(self.y_data, dim=0)

        # æ•°æ®æ ‡å‡†åŒ–
        self.mean = torch.mean(self.x_data, dim=(0, 2), keepdim=True)
        self.std = torch.std(self.x_data, dim=(0, 2), keepdim=True)
        self.x_data = (self.x_data - self.mean) / (self.std + 1e-8)

        # æœ€ç»ˆç»´åº¦éªŒè¯
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self)} ä¸ªæ ·æœ¬")
        print(f"ðŸ“ æ•°æ®å½¢çŠ¶: {self.x_data.shape}")
        assert self.x_data.dim() == 3, f"æ•°æ®ç»´åº¦é”™è¯¯: å½“å‰ç»´åº¦{self.x_data.dim()}Dï¼Œåº”ä¸º3D"
        assert self.x_data.shape[1] == 1, f"é€šé“ç»´åº¦é”™è¯¯: å½“å‰{self.x_data.shape[1]}ï¼Œåº”ä¸º1"

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        x = self.x_data[idx].clone()
        y = self.y_data[idx]

        # æ•°æ®å¢žå¼º
        if self.augment:
            # éšæœºç¿»è½¬
            if random.random() > 0.5:
                x = torch.flip(x, [-1])
            # æ·»åŠ é«˜æ–¯å™ªå£°
            if random.random() > 0.5:
                x += torch.randn_like(x) * 0.05

        return x.float(), y.long()


class NestedCVSplitter:
    def __init__(self, data_dir, n_splits=5, seed=42):
        # è·¯å¾„å¤„ç†
        self.data_dir = os.path.normpath(data_dir)
        print(f"\nðŸ” æ­£åœ¨æ‰«ææ•°æ®ç›®å½•: {self.data_dir}")

        # èŽ·å–æœ‰æ•ˆæ–‡ä»¶åˆ—è¡¨
        self.all_files = []
        for fname in os.listdir(self.data_dir):
            file_path = os.path.join(self.data_dir, fname)
            if fname.endswith('.npz') and os.path.isfile(file_path):
                if os.path.getsize(file_path) > 1024:  # 1KB
                    self.all_files.append(file_path)
                else:
                    print(f"  âš ï¸ å¿½ç•¥ç©ºæ–‡ä»¶: {fname}")

        if not self.all_files:
            raise FileNotFoundError(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„.npzæ–‡ä»¶: {self.data_dir}")

        print(f"ðŸ“‚ æ‰¾åˆ° {len(self.all_files)} ä¸ªæœ‰æ•ˆæ•°æ®æ–‡ä»¶")
        self.n_splits = n_splits
        self.kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)

    def get_fold(self, fold_idx):
        # å¤–å±‚åˆ’åˆ†
        splits = list(self.kfold.split(self.all_files))
        if fold_idx >= len(splits):
            raise ValueError(f"foldç´¢å¼•{fold_idx}è¶…å‡ºèŒƒå›´(æ€»foldæ•°{len(splits)})")

        train_val_indices, test_indices = splits[fold_idx]

        # æ·»åŠ è·¯å¾„éªŒè¯
        test_files = [self.all_files[i] for i in test_indices]
        if not test_files:
            raise ValueError(f"ç¬¬{fold_idx + 1}æŠ˜æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•é›†æ–‡ä»¶")

        # å†…å±‚åˆ’åˆ†
        inner_kfold = KFold(n_splits=self.n_splits - 1, shuffle=True, random_state=fold_idx)
        inner_splits = []

        for inner_train_idx, inner_val_idx in inner_kfold.split(train_val_indices):
            # è½¬æ¢åˆ°åŽŸå§‹ç´¢å¼•
            real_train = [train_val_indices[i] for i in inner_train_idx]
            real_val = [train_val_indices[i] for i in inner_val_idx]

            inner_splits.append({
                'train_files': [self.all_files[i] for i in real_train],
                'val_files': [self.all_files[i] for i in real_val]
            })

        return {
            'test_files': [self.all_files[i] for i in test_indices],
            'train_val_splits': inner_splits
        }


def create_loaders(train_files, val_files, test_files, batch_size=32):
    # æ·»åŠ ç©ºåˆ—è¡¨æ£€æŸ¥
    def check_files(file_list, name):
        # å…è®¸æµ‹è¯•é˜¶æ®µçš„ç©ºåˆ—è¡¨
        if not file_list and name != "æµ‹è¯•é›†":
            raise ValueError(f"{name}æ–‡ä»¶åˆ—è¡¨ä¸ºç©ºï¼")
        valid_files = [f for f in file_list if os.path.exists(f)]
        if not valid_files and name == "æµ‹è¯•é›†":
            raise ValueError("æµ‹è¯•é›†ä¸èƒ½ä¸ºç©º")
        return valid_files

    # è®­ç»ƒé›†å’ŒéªŒè¯é›†å…è®¸ä¸ºç©ºï¼ˆä»…åœ¨æµ‹è¯•é˜¶æ®µï¼‰
    train_set = None
    if train_files:
        train_set = SleepDataset(check_files(train_files, "è®­ç»ƒé›†"), augment=True)

    val_set = None
    if val_files:
        val_set = SleepDataset(check_files(val_files, "éªŒè¯é›†"), augment=False)

    # æµ‹è¯•é›†å¿…é¡»å­˜åœ¨
    test_set = SleepDataset(check_files(test_files, "æµ‹è¯•é›†"), augment=False)

    loader_args = {
        'batch_size': batch_size,
        'num_workers': 4 if os.cpu_count() > 4 else 2,
        'pin_memory': True,
        'persistent_workers': True
    }

    return (
        DataLoader(train_set, shuffle=True, **loader_args) if train_set else None,
        DataLoader(val_set, shuffle=False, **loader_args) if val_set else None,
        DataLoader(test_set, shuffle=False, **loader_args)
    )