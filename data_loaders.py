import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold
import random

class SleepDataset(Dataset):
    def __init__(self, file_list, augment=True):
        self.augment = augment
        self.x_data = []
        self.y_data = []

        print(f"\nğŸ”§ æ­£åœ¨åŠ è½½ {len(file_list)} ä¸ªæ•°æ®æ–‡ä»¶...")
        valid_count = 0

        for idx, file in enumerate(file_list, 1):
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(file):
                    print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file}")
                    continue

                # åŠ è½½æ•°æ®
                with np.load(file) as data:
                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    if 'x' not in data or 'y' not in data:
                        print(f"  âš ï¸ æ–‡ä»¶æ ¼å¼é”™è¯¯: {file} ç¼ºå°‘x/yå­—æ®µ")
                        continue

                    x = data['x'].astype(np.float32)
                    y = data['y'].astype(np.int64)

                    # ç»Ÿä¸€ç»´åº¦å¤„ç†
                    if x.ndim == 3:
                        # æƒ…å†µ1: (n, 1, 3000) -> ç›´æ¥ä½¿ç”¨
                        if x.shape[1] == 1 and x.shape[2] == 3000:
                            pass
                        # æƒ…å†µ2: (n, 3000, 1) -> è½¬ç½®ä¸º(n, 1, 3000)
                        elif x.shape[1] == 3000 and x.shape[2] == 1:
                            x = x.transpose(0, 2, 1)
                        else:
                            raise ValueError(f"éå¸¸è§„ä¸‰ç»´æ•°æ®: {x.shape}")
                    # å¤„ç†äºŒç»´æ•°æ® (n, 3000) -> (n, 1, 3000)
                    elif x.ndim == 2:
                        x = x[:, np.newaxis, :]
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç»´åº¦: {x.ndim}")

                    # æœ€ç»ˆå½¢çŠ¶éªŒè¯
                    if x.shape[1:] != (1, 3000):
                        print(f"  âš ï¸ ç»´åº¦æ ¡éªŒå¤±è´¥: {file} å½¢çŠ¶={x.shape}")
                        continue

                    # è½¬æ¢ä¸ºTensor
                    self.x_data.append(torch.from_numpy(x))
                    self.y_data.append(torch.from_numpy(y))
                    valid_count += 1

                    # è¿›åº¦æ˜¾ç¤º
                    if idx % 5 == 0:
                        print(f"  å·²åŠ è½½ {idx}/{len(file_list)} ä¸ªæ–‡ä»¶...")

            except Exception as e:
                print(f"  âŒ åŠ è½½é”™è¯¯ {file}: {str(e)}")
                continue

        # æ£€æŸ¥æœ‰æ•ˆæ•°æ®
        if valid_count == 0:
            raise RuntimeError(f"âŒ æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        self.x_data = torch.cat(self.x_data, dim=0)
        self.y_data = torch.cat(self.y_data, dim=0)

        # æ•°æ®æ ‡å‡†åŒ–
        self.mean = torch.mean(self.x_data, dim=(0, 2), keepdim=True)
        self.std = torch.std(self.x_data, dim=(0, 2), keepdim=True)
        self.x_data = (self.x_data - self.mean) / (self.std + 1e-8)

        # æœ€ç»ˆç»´åº¦éªŒè¯
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ æ•°æ®å½¢çŠ¶: {self.x_data.shape}")
        assert self.x_data.dim() == 3, f"æ•°æ®ç»´åº¦é”™è¯¯: å½“å‰ç»´åº¦{self.x_data.dim()}Dï¼Œåº”ä¸º3D"
        assert self.x_data.shape[1] == 1, f"é€šé“ç»´åº¦é”™è¯¯: å½“å‰{self.x_data.shape[1]}ï¼Œåº”ä¸º1"

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        x = self.x_data[idx].clone()
        y = self.y_data[idx]

        # æ•°æ®å¢å¼º
        if self.augment:
            # éšæœºç¿»è½¬
            if random.random() > 0.5:
                x = torch.flip(x, [-1])
            # æ·»åŠ é«˜æ–¯å™ªå£°
            if random.random() > 0.5:
                x += torch.randn_like(x) * 0.05

        return x.float(), y.long()


class NestedCVSplitter:
    def __init__(self, data_dir, n_splits=5, seed=42):
        # è·¯å¾„å¤„ç†
        self.data_dir = os.path.normpath(data_dir)
        print(f"\nğŸ” æ­£åœ¨æ‰«ææ•°æ®ç›®å½•: {self.data_dir}")

        # è·å–æœ‰æ•ˆæ–‡ä»¶åˆ—è¡¨
        self.all_files = []
        for fname in os.listdir(self.data_dir):
            file_path = os.path.join(self.data_dir, fname)
            if fname.endswith('.npz') and os.path.isfile(file_path):
                if os.path.getsize(file_path) > 1024:  # 1KB
                    self.all_files.append(file_path)
                else:
                    print(f"  âš ï¸ å¿½ç•¥ç©ºæ–‡ä»¶: {fname}")

        if not self.all_files:
            raise FileNotFoundError(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„.npzæ–‡ä»¶: {self.data_dir}")

        print(f"ğŸ“‚ æ‰¾åˆ° {len(self.all_files)} ä¸ªæœ‰æ•ˆæ•°æ®æ–‡ä»¶")
        self.n_splits = n_splits
        self.kfold = KFold(n_splits=n_splits, shuffle=True, random_state=seed)

    def get_fold(self, fold_idx):
        assert 0 <= fold_idx < self.n_splits, f"æ— æ•ˆçš„foldç´¢å¼•: {fold_idx}"

        # å¤–å±‚åˆ’åˆ†
        outer_gen = self.kfold.split(self.all_files)
        train_val_indices, test_indices = next(
            (x for i, x in enumerate(outer_gen) if i == fold_idx),
            (None, None)
        )

        # å†…å±‚åˆ’åˆ†
        inner_kfold = KFold(n_splits=self.n_splits - 1, shuffle=True, random_state=fold_idx)
        inner_splits = []

        for inner_train_idx, inner_val_idx in inner_kfold.split(train_val_indices):
            # è½¬æ¢åˆ°åŸå§‹ç´¢å¼•
            real_train = [train_val_indices[i] for i in inner_train_idx]
            real_val = [train_val_indices[i] for i in inner_val_idx]

            inner_splits.append({
                'train_files': [self.all_files[i] for i in real_train],
                'val_files': [self.all_files[i] for i in real_val]
            })

        return {
            'test_files': [self.all_files[i] for i in test_indices],
            'train_val_splits': inner_splits
        }


def create_loaders(train_files, val_files, test_files, batch_size=32):
    # è®­ç»ƒé›†å¸¦å¢å¼º
    train_set = SleepDataset(train_files, augment=True)

    # éªŒè¯/æµ‹è¯•é›†ä¸å¸¦å¢å¼º
    val_set = SleepDataset(val_files, augment=False)
    test_set = SleepDataset(test_files, augment=False)

    loader_args = {
        'batch_size': batch_size,
        'num_workers': 0 if os.name == 'nt' else 4,  # Windowsç³»ç»Ÿç¦ç”¨å¤šè¿›ç¨‹
        'pin_memory': torch.cuda.is_available(),
        'persistent_workers': False
    }

    train_loader = DataLoader(train_set, shuffle=True, **loader_args)
    val_loader = DataLoader(val_set, shuffle=False, **loader_args)
    test_loader = DataLoader(test_set, shuffle=False, **loader_args)

    return train_loader, val_loader, test_loader